{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET training - Pytorch 2.1\n",
    "This notebook shows how to fine-tune a pretrained UNET PyTorch model with AWS Trainium (trn1 instances) using NeuronSDK.\\\n",
    "The model implementation is provided by milesial/Pytorch-UNet. \n",
    "\n",
    "\n",
    "\n",
    "The example has 2 stages:\n",
    "1. First compile the model using the utility `neuron_parallel_compile` to compile the model to run on the AWS Trainium device.\n",
    "1. Run the fine-tuning script to train the model based on image segmentaion task. The training job will use 32 workers with data parallel to speed up the training.\n",
    "\n",
    "It has been tested and run on trn1.32xlarge instance using 256 x 256 input image for binary segmentation with batch size 4.\n",
    "\n",
    "**Reference:** \n",
    "milesial, U-Net: Semantic segmentation with PyTorch, GitHub repository\n",
    "https://github.com/milesial/Pytorch-UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-neuronx-runtime-discovery==2.9\n",
      "libneuronxla==0.5.971\n",
      "neuronx-cc==2.13.72.0+78a426937\n",
      "neuronx-distributed==0.7.0\n",
      "pillow==10.3.0\n",
      "scikit-learn==1.3.2\n",
      "scipy==1.10.1\n",
      "tensorboard==2.14.0\n",
      "tensorboard-data-server==0.7.2\n",
      "tensorboard-plugin-neuronx==2.6.7.0\n",
      "timm==0.9.16\n",
      "torch==1.13.1\n",
      "torch-neuronx==1.13.1.1.14.0\n",
      "torch-xla==1.13.1+torchneurone\n",
      "torchvision==0.14.1\n",
      "transformers==4.40.1\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep -E 'neuron|torch|pill|glob|sci|timm|transformers|tensorboard'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Neuron Compiler and Neuron/XLA packages\n",
    "%pip install -U \"timm\" \"tensorboard\" torchvision==0.16.*\n",
    "%pip install -U \"Pillow\" \"glob2\" \"scikit-learn\" \n",
    "# use --force-reinstall if you're facing some issues while loading the modules\n",
    "# now restart the kernel again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Download Carvana dataset\n",
    "This example uses Carvana dataset which requires users to manually download the dataset before training.\\\n",
    " https://www.kaggle.com/competitions/carvana-image-masking-challenge/data \n",
    "\n",
    "1. Download train.zip and train_masks.zip \n",
    "2. Unzip\n",
    "3. Create a carvana directory\n",
    "4. Directory structure\\\n",
    "carvana/train/\\\n",
    "carvana/train_masks/\n",
    "\n",
    "dataset_path = \\<Path to Carvana directory\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_workers = 32\n",
    "num_workers = 2\n",
    "dataloader_num_workers = 2\n",
    "image_dim = 256\n",
    "# num_epochs = 20\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-4\n",
    "batch_size = 4\n",
    "env_var_options = \"NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS=3  \" + \\\n",
    "    \"NEURON_CC_FLAGS=\\'--cache_dir=./compiler_cache --model-type=cnn-training\\'\"\n",
    "dataset_path = \"./carvana/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Compile the model with neuron_parallel_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS=3  NEURON_CC_FLAGS='--cache_dir=./compiler_cache --model-type=cnn-training' neuron_parallel_compile torchrun --nproc_per_node=2    train.py     --num_workers 2     --image_dim 256     --num_epochs 2     --batch_size 4     --drop_last     --data_dir ./carvana/     --lr 0.0002\n"
     ]
    }
   ],
   "source": [
    "COMPILE_CMD = f\"\"\"{env_var_options} neuron_parallel_compile torchrun --nproc_per_node={num_workers} \\\n",
    "   train.py \\\n",
    "    --num_workers {dataloader_num_workers} \\\n",
    "    --image_dim {image_dim} \\\n",
    "    --num_epochs 2 \\\n",
    "    --batch_size {batch_size} \\\n",
    "    --drop_last \\\n",
    "    --data_dir {dataset_path} \\\n",
    "    --lr {learning_rate}\"\"\"\n",
    "print(COMPILE_CMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS=3  NEURON_CC_FLAGS='--cache_dir=./compiler_cache --model-type=cnn-training' neuron_parallel_compile torchrun --nproc_per_node=32 train.py --num_workers 2 --image_dim 256 --num_epochs 2 --batch_size 4 --drop_last --data_dir ./carvana/ --lr 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile model\n",
      "Running command: \n",
      "NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS=3  NEURON_CC_FLAGS='--cache_dir=./compiler_cache --model-type=cnn-training' neuron_parallel_compile torchrun --nproc_per_node=2    train.py     --num_workers 2     --image_dim 256     --num_epochs 2     --batch_size 4     --drop_last     --data_dir ./carvana/     --lr 0.0002\n",
      "2024-05-03 20:42:14.000627:  1253951  INFO ||NEURON_PARALLEL_COMPILE||: Removing existing workdir /tmp/ubuntu/parallel_compile_workdir\n",
      "2024-05-03 20:42:14.000627:  1253951  INFO ||NEURON_PARALLEL_COMPILE||: Running trial run (add option to terminate trial run early; also ignore trial run's generated outputs, i.e. loss, checkpoints)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "==> Preparing data..\n",
      "==> Preparing data..\n",
      "train_dataset : 4579, test_dataset : 509\n",
      "Image shape : torch.Size([3, 256, 256]), mask shape : torch.Size([1, 256, 256])\n",
      "Epoch 1 train begin 2024-05-03 20:42:29.472695\n",
      "2024-05-03 20:42:29.000699:  1254571  INFO ||NEURON_CACHE||: Compile cache path: ./compiler_cache\n",
      "2024-05-03 20:42:29.000700:  1254571  INFO ||NEURON_CC_WRAPPER||: Extracting graphs (/home/ubuntu/torch_neuronx_exploration/from_samples/unet/compiler_cache/neuronxcc-2.13.72.0+78a426937/MODULE_1839587966621543786+ade7b014/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      "2024-05-03 20:42:30.000259:  1254679  INFO ||NEURON_CACHE||: Compile cache path: ./compiler_cache\n",
      "2024-05-03 20:42:30.000261:  1254679  INFO ||NEURON_CC_WRAPPER||: Extracting graphs (/home/ubuntu/torch_neuronx_exploration/from_samples/unet/compiler_cache/neuronxcc-2.13.72.0+78a426937/MODULE_3527785839559766665+ade7b014/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      "| Training Device=xla:1 Epoch=1 Step=0 Learning_Rate=0.0002 Loss=0.00000 Throughput=13.94810 Time=2024-05-03 20:42:30.327264\n",
      "2024-05-03 20:42:30.000545:  1254689  INFO ||NEURON_CACHE||: Compile cache path: ./compiler_cache\n",
      "2024-05-03 20:42:30.000547:  1254689  INFO ||NEURON_CC_WRAPPER||: Extracting graphs (/home/ubuntu/torch_neuronx_exploration/from_samples/unet/compiler_cache/neuronxcc-2.13.72.0+78a426937/MODULE_17722205062385404385+ade7b014/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      "2024-05-03 20:42:31.000195:  1254737  INFO ||NEURON_CACHE||: Compile cache path: ./compiler_cache\n",
      "2024-05-03 20:42:31.000197:  1254737  INFO ||NEURON_CC_WRAPPER||: Extracting graphs (/home/ubuntu/torch_neuronx_exploration/from_samples/unet/compiler_cache/neuronxcc-2.13.72.0+78a426937/MODULE_8208368104965975205+ade7b014/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      "| Training Device=xla:1 Epoch=1 Step=20 Learning_Rate=0.0002 Loss=0.00000 Throughput=260.82257 Time=2024-05-03 20:42:31.953624\n",
      "| Training Device=xla:1 Epoch=1 Step=40 Learning_Rate=0.0002 Loss=0.00000 Throughput=84.92974 Time=2024-05-03 20:42:33.569687\n",
      "| Training Device=xla:1 Epoch=1 Step=60 Learning_Rate=0.0002 Loss=0.00000 Throughput=90.45989 Time=2024-05-03 20:42:35.155689\n",
      "| Training Device=xla:1 Epoch=1 Step=80 Learning_Rate=0.0002 Loss=0.00000 Throughput=85.47382 Time=2024-05-03 20:42:36.771453\n",
      "| Training Device=xla:1 Epoch=1 Step=100 Learning_Rate=0.0002 Loss=-0.00020 Throughput=89.75767 Time=2024-05-03 20:42:38.336772\n",
      "| Training Device=xla:1 Epoch=1 Step=120 Learning_Rate=0.0002 Loss=0.24332 Throughput=83.37737 Time=2024-05-03 20:42:39.988567\n",
      "| Training Device=xla:1 Epoch=1 Step=140 Learning_Rate=0.0002 Loss=-0.00020 Throughput=81.86170 Time=2024-05-03 20:42:41.667046\n",
      "| Training Device=xla:1 Epoch=1 Step=160 Learning_Rate=0.0002 Loss=0.28553 Throughput=87.58604 Time=2024-05-03 20:42:43.269270\n",
      "| Training Device=xla:1 Epoch=1 Step=180 Learning_Rate=0.0002 Loss=0.28553 Throughput=93.92481 Time=2024-05-03 20:42:44.838357\n",
      "| Training Device=xla:1 Epoch=1 Step=200 Learning_Rate=0.0002 Loss=0.28553 Throughput=86.24197 Time=2024-05-03 20:42:46.479474\n",
      "| Training Device=xla:1 Epoch=1 Step=220 Learning_Rate=0.0002 Loss=0.28553 Throughput=85.75851 Time=2024-05-03 20:42:48.188180\n",
      "| Training Device=xla:1 Epoch=1 Step=240 Learning_Rate=0.0002 Loss=0.28553 Throughput=83.10329 Time=2024-05-03 20:42:49.823722\n",
      "| Training Device=xla:1 Epoch=1 Step=260 Learning_Rate=0.0002 Loss=0.28553 Throughput=88.57883 Time=2024-05-03 20:42:51.396360\n",
      "| Training Device=xla:1 Epoch=1 Step=280 Learning_Rate=0.0002 Loss=0.28553 Throughput=87.94720 Time=2024-05-03 20:42:53.021245\n",
      "| Training Device=xla:1 Epoch=1 Step=300 Learning_Rate=0.0002 Loss=0.28553 Throughput=77.71947 Time=2024-05-03 20:42:54.734300\n",
      "| Training Device=xla:1 Epoch=1 Step=320 Learning_Rate=0.0002 Loss=0.28553 Throughput=79.03467 Time=2024-05-03 20:42:56.495026\n",
      "| Training Device=xla:1 Epoch=1 Step=340 Learning_Rate=0.0002 Loss=0.28553 Throughput=91.47201 Time=2024-05-03 20:42:58.035542\n",
      "| Training Device=xla:1 Epoch=1 Step=360 Learning_Rate=0.0002 Loss=0.28553 Throughput=91.47997 Time=2024-05-03 20:42:59.587760\n",
      "| Training Device=xla:1 Epoch=1 Step=380 Learning_Rate=0.0002 Loss=0.28553 Throughput=92.64742 Time=2024-05-03 20:43:01.181369\n",
      "| Training Device=xla:1 Epoch=1 Step=400 Learning_Rate=0.0002 Loss=0.28553 Throughput=90.97753 Time=2024-05-03 20:43:02.706870\n",
      "| Training Device=xla:1 Epoch=1 Step=420 Learning_Rate=0.0002 Loss=0.28553 Throughput=87.21365 Time=2024-05-03 20:43:04.323890\n",
      "| Training Device=xla:1 Epoch=1 Step=440 Learning_Rate=0.0002 Loss=0.28553 Throughput=84.00358 Time=2024-05-03 20:43:05.952783\n",
      "| Training Device=xla:1 Epoch=1 Step=460 Learning_Rate=0.0002 Loss=0.28553 Throughput=93.82693 Time=2024-05-03 20:43:07.453576\n",
      "| Training Device=xla:1 Epoch=1 Step=480 Learning_Rate=0.0002 Loss=0.28553 Throughput=86.30131 Time=2024-05-03 20:43:09.051551\n",
      "| Training Device=xla:1 Epoch=1 Step=500 Learning_Rate=0.0002 Loss=0.28553 Throughput=88.63955 Time=2024-05-03 20:43:10.708118\n",
      "| Training Device=xla:1 Epoch=1 Step=520 Learning_Rate=0.0002 Loss=0.57662 Throughput=86.85880 Time=2024-05-03 20:43:12.369203\n",
      "| Training Device=xla:1 Epoch=1 Step=540 Learning_Rate=0.0002 Loss=0.28553 Throughput=90.54474 Time=2024-05-03 20:43:14.052967\n",
      "| Training Device=xla:1 Epoch=1 Step=560 Learning_Rate=0.0002 Loss=0.28553 Throughput=82.74913 Time=2024-05-03 20:43:15.718919\n",
      "Epoch 1 train end 2024-05-03 20:43:16.425665\n",
      "Average train throughput: 103.3276\n",
      "Max train throughput: 266.1759\n",
      "Epoch 2 train begin 2024-05-03 20:43:16.425932\n",
      "| Training Device=xla:1 Epoch=2 Step=0 Learning_Rate=0.0002 Loss=1.51821 Throughput=75.35010 Time=2024-05-03 20:43:16.845271\n",
      "| Training Device=xla:1 Epoch=2 Step=20 Learning_Rate=0.0002 Loss=-0.00000 Throughput=97.90386 Time=2024-05-03 20:43:18.248928\n",
      "| Training Device=xla:1 Epoch=2 Step=40 Learning_Rate=0.0002 Loss=0.00000 Throughput=98.70957 Time=2024-05-03 20:43:19.679782\n",
      "| Training Device=xla:1 Epoch=2 Step=60 Learning_Rate=0.0002 Loss=0.00000 Throughput=98.53414 Time=2024-05-03 20:43:21.092496\n",
      "| Training Device=xla:1 Epoch=2 Step=80 Learning_Rate=0.0002 Loss=0.00000 Throughput=111.06955 Time=2024-05-03 20:43:22.397702\n",
      "| Training Device=xla:1 Epoch=2 Step=100 Learning_Rate=0.0002 Loss=0.00000 Throughput=97.04356 Time=2024-05-03 20:43:23.841095\n",
      "| Training Device=xla:1 Epoch=2 Step=120 Learning_Rate=0.0002 Loss=0.00000 Throughput=101.17633 Time=2024-05-03 20:43:25.243302\n",
      "| Training Device=xla:1 Epoch=2 Step=140 Learning_Rate=0.0002 Loss=0.00000 Throughput=101.56081 Time=2024-05-03 20:43:26.642745\n",
      "| Training Device=xla:1 Epoch=2 Step=160 Learning_Rate=0.0002 Loss=0.00000 Throughput=104.05691 Time=2024-05-03 20:43:28.009778\n",
      "| Training Device=xla:1 Epoch=2 Step=180 Learning_Rate=0.0002 Loss=0.68612 Throughput=100.95764 Time=2024-05-03 20:43:29.433740\n",
      "| Training Device=xla:1 Epoch=2 Step=200 Learning_Rate=0.0002 Loss=0.00000 Throughput=104.60315 Time=2024-05-03 20:43:30.809048\n",
      "| Training Device=xla:1 Epoch=2 Step=220 Learning_Rate=0.0002 Loss=0.70109 Throughput=99.97602 Time=2024-05-03 20:43:32.232415\n",
      "| Training Device=xla:1 Epoch=2 Step=240 Learning_Rate=0.0002 Loss=0.70824 Throughput=99.50867 Time=2024-05-03 20:43:33.647731\n",
      "| Training Device=xla:1 Epoch=2 Step=260 Learning_Rate=0.0002 Loss=0.71449 Throughput=103.40581 Time=2024-05-03 20:43:35.035500\n",
      "| Training Device=xla:1 Epoch=2 Step=280 Learning_Rate=0.0002 Loss=0.72024 Throughput=95.81749 Time=2024-05-03 20:43:36.475775\n",
      "| Training Device=xla:1 Epoch=2 Step=300 Learning_Rate=0.0002 Loss=-0.00000 Throughput=93.01058 Time=2024-05-03 20:43:37.927086\n",
      "| Training Device=xla:1 Epoch=2 Step=320 Learning_Rate=0.0002 Loss=0.72455 Throughput=104.28051 Time=2024-05-03 20:43:39.341352\n",
      "| Training Device=xla:1 Epoch=2 Step=340 Learning_Rate=0.0002 Loss=0.00000 Throughput=102.06674 Time=2024-05-03 20:43:40.783279\n",
      "| Training Device=xla:1 Epoch=2 Step=360 Learning_Rate=0.0002 Loss=0.00000 Throughput=92.03237 Time=2024-05-03 20:43:42.255479\n",
      "| Training Device=xla:1 Epoch=2 Step=380 Learning_Rate=0.0002 Loss=0.00000 Throughput=89.85297 Time=2024-05-03 20:43:43.738235\n",
      "| Training Device=xla:1 Epoch=2 Step=400 Learning_Rate=0.0002 Loss=0.00000 Throughput=97.10268 Time=2024-05-03 20:43:45.206712\n",
      "| Training Device=xla:1 Epoch=2 Step=420 Learning_Rate=0.0002 Loss=0.00000 Throughput=101.52691 Time=2024-05-03 20:43:46.604899\n",
      "| Training Device=xla:1 Epoch=2 Step=440 Learning_Rate=0.0002 Loss=0.00000 Throughput=95.98400 Time=2024-05-03 20:43:48.071204\n",
      "| Training Device=xla:1 Epoch=2 Step=460 Learning_Rate=0.0002 Loss=0.00000 Throughput=102.59271 Time=2024-05-03 20:43:49.476617\n",
      "| Training Device=xla:1 Epoch=2 Step=480 Learning_Rate=0.0002 Loss=0.00000 Throughput=89.49605 Time=2024-05-03 20:43:50.972937\n",
      "| Training Device=xla:1 Epoch=2 Step=500 Learning_Rate=0.0002 Loss=0.00000 Throughput=100.14628 Time=2024-05-03 20:43:52.454522\n",
      "| Training Device=xla:1 Epoch=2 Step=520 Learning_Rate=0.0002 Loss=0.00000 Throughput=102.63337 Time=2024-05-03 20:43:53.841030\n",
      "| Training Device=xla:1 Epoch=2 Step=540 Learning_Rate=0.0002 Loss=0.00000 Throughput=102.83928 Time=2024-05-03 20:43:55.210255\n",
      "| Training Device=xla:1 Epoch=2 Step=560 Learning_Rate=0.0002 Loss=0.00000 Throughput=105.47058 Time=2024-05-03 20:43:56.603401\n",
      "Epoch 2 train end 2024-05-03 20:43:57.220016\n",
      "Average train throughput: 108.7915\n",
      "Max train throughput: 266.1759\n",
      "TrainLoss: 1.5182\n",
      "TrainRuntime 1.4625 minutes\n",
      "2024-05-03 20:44:01.000916:  1253951  INFO ||NEURON_PARALLEL_COMPILE||: New graph list from script: /home/ubuntu/torch_neuronx_exploration/from_samples/unet/compiler_cache/neuronxcc-2.13.72.0+78a426937/MODULE_1839587966621543786+ade7b014/model.hlo.pb\n",
      "\t/home/ubuntu/torch_neuronx_exploration/from_samples/unet/compiler_cache/neuronxcc-2.13.72.0+78a426937/MODULE_3527785839559766665+ade7b014/model.hlo.pb\n",
      "\t/home/ubuntu/torch_neuronx_exploration/from_samples/unet/compiler_cache/neuronxcc-2.13.72.0+78a426937/MODULE_17722205062385404385+ade7b014/model.hlo.pb\n",
      "\t/home/ubuntu/torch_neuronx_exploration/from_samples/unet/compiler_cache/neuronxcc-2.13.72.0+78a426937/MODULE_8208368104965975205+ade7b014/model.hlo.pb\n",
      "2024-05-03 20:44:01.000916:  1253951  INFO ||NEURON_CACHE||: Compile cache path: ./compiler_cache\n",
      "2024-05-03 20:44:01.000927:  1291285  INFO ||NEURON_CACHE||: Current remaining items are 0, locked are 4, failed are 0, done are 0, total is 4\n",
      "2024-05-03 20:44:01.000927:  1291286  INFO ||NEURON_CACHE||: Current remaining items are 0, locked are 4, failed are 0, done are 0, total is 4\n",
      "2024-05-03 20:44:01.000927:  1291280  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/69564426-0585-465f-81dc-d477c6b46ecc/model.MODULE_1839587966621543786+ade7b014.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/69564426-0585-465f-81dc-d477c6b46ecc/model.MODULE_1839587966621543786+ade7b014.neff', '--model-type=cnn-training', '--verbose=35']\n",
      "2024-05-03 20:44:01.000927:  1291287  INFO ||NEURON_CACHE||: Current remaining items are 0, locked are 4, failed are 0, done are 0, total is 4\n",
      "2024-05-03 20:44:01.000927:  1291282  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/57e52def-76a0-4670-b9a5-24d0c0187f1f/model.MODULE_3527785839559766665+ade7b014.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/57e52def-76a0-4670-b9a5-24d0c0187f1f/model.MODULE_3527785839559766665+ade7b014.neff', '--model-type=cnn-training', '--verbose=35']\n",
      "2024-05-03 20:44:01.000928:  1291284  INFO ||NEURON_CACHE||: Current remaining items are 0, locked are 4, failed are 0, done are 0, total is 4\n",
      "2024-05-03 20:44:01.000928:  1291283  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/3011898a-69c1-4dca-88ae-a066ae338f4b/model.MODULE_17722205062385404385+ade7b014.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/3011898a-69c1-4dca-88ae-a066ae338f4b/model.MODULE_17722205062385404385+ade7b014.neff', '--model-type=cnn-training', '--verbose=35']\n",
      "2024-05-03 20:44:01.000928:  1291281  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/ubuntu/neuroncc_compile_workdir/e9d3df4f-d73b-4fc9-a6b6-a43f5feeaf6d/model.MODULE_8208368104965975205+ade7b014.hlo.pb', '--output', '/tmp/ubuntu/neuroncc_compile_workdir/e9d3df4f-d73b-4fc9-a6b6-a43f5feeaf6d/model.MODULE_8208368104965975205+ade7b014.neff', '--model-type=cnn-training', '--verbose=35']\n",
      "....\n",
      "Compiler status PASS\n",
      "2024-05-03 20:44:04.000450:  1291280  INFO ||NEURON_CACHE||: Current remaining items are 0, locked are 3, failed are 0, done are 1, total is 4\n",
      "......\n",
      "Compiler status PASS\n",
      "2024-05-03 20:44:57.000705:  1291282  INFO ||NEURON_CACHE||: Current remaining items are 0, locked are 2, failed are 0, done are 2, total is 4\n",
      "..............................\n",
      "Compiler status PASS\n",
      "2024-05-03 20:49:58.000594:  1291283  INFO ||NEURON_CACHE||: Current remaining items are 0, locked are 1, failed are 0, done are 3, total is 4\n",
      "...............\n",
      "Compiler status PASS\n",
      "2024-05-03 20:55:02.000096:  1291281  INFO ||NEURON_CACHE||: Current remaining items are 0, locked are 0, failed are 0, done are 4, total is 4\n",
      "2024-05-03 20:55:02.000099:  1253951  INFO ||NEURON_PARALLEL_COMPILE||: {\n",
      "    \"compilation_summary\": {\n",
      "        \"true\": 4\n",
      "    },\n",
      "    \"compilation_report\": {\n",
      "        \"/home/ubuntu/torch_neuronx_exploration/from_samples/unet/compiler_cache/neuronxcc-2.13.72.0+78a426937/MODULE_1839587966621543786+ade7b014/model.hlo.pb\": {\n",
      "            \"status\": true,\n",
      "            \"retry\": 0,\n",
      "            \"compile_time\": 2.5237348079681396\n",
      "        },\n",
      "        \"/home/ubuntu/torch_neuronx_exploration/from_samples/unet/compiler_cache/neuronxcc-2.13.72.0+78a426937/MODULE_8208368104965975205+ade7b014/model.hlo.pb\": {\n",
      "            \"status\": true,\n",
      "            \"retry\": 0,\n",
      "            \"compile_time\": 660.1692728996277\n",
      "        },\n",
      "        \"/home/ubuntu/torch_neuronx_exploration/from_samples/unet/compiler_cache/neuronxcc-2.13.72.0+78a426937/MODULE_3527785839559766665+ade7b014/model.hlo.pb\": {\n",
      "            \"status\": true,\n",
      "            \"retry\": 0,\n",
      "            \"compile_time\": 55.778361797332764\n",
      "        },\n",
      "        \"/home/ubuntu/torch_neuronx_exploration/from_samples/unet/compiler_cache/neuronxcc-2.13.72.0+78a426937/MODULE_17722205062385404385+ade7b014/model.hlo.pb\": {\n",
      "            \"status\": true,\n",
      "            \"retry\": 0,\n",
      "            \"compile_time\": 356.6670322418213\n",
      "        }\n",
      "    },\n",
      "    \"start_time\": 1714769041.9164414,\n",
      "    \"compilation_time\": 660.182906627655\n",
      "}\n",
      "2024-05-03 20:55:02.000099:  1253951  INFO ||NEURON_PARALLEL_COMPILE||: Total graphs: 4\n",
      "2024-05-03 20:55:02.000099:  1253951  INFO ||NEURON_PARALLEL_COMPILE||: Total successful compilations: 4\n",
      "2024-05-03 20:55:02.000099:  1253951  INFO ||NEURON_PARALLEL_COMPILE||: Total failed compilations: 0\n",
      "Compilation Success!!!\n",
      "CPU times: user 98.6 ms, sys: 25.6 ms, total: 124 ms\n",
      "Wall time: 12min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import subprocess\n",
    "print(\"Compile model\")\n",
    "COMPILE_CMD = f\"\"\"{env_var_options} neuron_parallel_compile torchrun --nproc_per_node={num_workers} \\\n",
    "   train.py \\\n",
    "    --num_workers {dataloader_num_workers} \\\n",
    "    --image_dim {image_dim} \\\n",
    "    --num_epochs 2 \\\n",
    "    --batch_size {batch_size} \\\n",
    "    --drop_last \\\n",
    "    --data_dir {dataset_path} \\\n",
    "    --lr {learning_rate}\"\"\"\n",
    "\n",
    "print(f'Running command: \\n{COMPILE_CMD}')\n",
    "if subprocess.check_call(COMPILE_CMD,shell=True):\n",
    "   print(\"There was an error with the compilation command\")\n",
    "else:\n",
    "   print(\"Compilation Success!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Compile and Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import subprocess\n",
    "print(\"Compile model\")\n",
    "COMPILE_CMD = f\"\"\"{env_var_options} torchrun --nproc_per_node={num_workers} \\\n",
    "    train.py \\\n",
    "    --num_workers {dataloader_num_workers} \\\n",
    "    --image_dim {image_dim} \\\n",
    "    --num_epochs {num_epochs} \\\n",
    "    --batch_size {batch_size} \\\n",
    "    --do_eval \\\n",
    "    --drop_last \\\n",
    "    --data_dir {dataset_path} \\\n",
    "    --lr {learning_rate}\"\"\"\n",
    "\n",
    "print(f'Running command: \\n{COMPILE_CMD}')\n",
    "if subprocess.check_call(COMPILE_CMD,shell=True):\n",
    "   print(\"There was an error with the fine-tune command\")\n",
    "else:\n",
    "   print(\"Fine-tune Successful!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_torch_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
